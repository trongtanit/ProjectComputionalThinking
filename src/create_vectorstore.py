"""
Táº O VECTOR DATABASE - FINAL OPTIMIZED VERSION
- Gá»™p data (star.json) vÃ  Ä‘áº£m báº£o ID duy nháº¥t cho má»—i document (fix lá»—i 730 quÃ¡n)
- Embed specialties + category vÃ o content Ä‘á»ƒ tÄƒng cÆ°á»ng tÃ¬m kiáº¿m (Vector Search)
"""

import json
import os
import torch
import shutil
from pathlib import Path
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from typing import List, Dict

# ============= Cáº¤U HÃŒNH =============
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
VECTOR_DB_PATH = DATA_DIR / "chroma_db"
# Chá»‰ cáº§n trá» Ä‘áº¿n file star.json duy nháº¥t Ä‘Ã£ gá»™p data
JSON_FILE = DATA_DIR / "star.json" 

EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
EMBEDDING_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ============= FUNCTIONS =============

def load_documents(json_path: Path) -> List[Dict]:
    """Load dá»¯ liá»‡u tá»« file JSON (Ä‘Ã£ gá»™p)"""
    print(f"ğŸ“¥ Loading documents from: {json_path}")
    
    if not json_path.exists():
        raise FileNotFoundError(f"File khÃ´ng tá»“n táº¡i: {json_path}")
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Xá»­ lÃ½ linh hoáº¡t cho cáº£ format {documents: [...]} vÃ  format [...]
    if isinstance(data, dict) and 'documents' in data:
        documents = data['documents']
    elif isinstance(data, list):
        documents = data
    else:
        raise ValueError("JSON format khÃ´ng há»£p lá»‡!")
    
    print(f"âœ… Loaded {len(documents)} documents (dá»± kiáº¿n khoáº£ng 830)")
    return documents


def create_langchain_documents(documents: List[Dict]) -> List[Document]:
    """
    Chuyá»ƒn Ä‘á»•i documents sang LangChain Document format.
    âœ… Tá»‘i Æ°u: Äáº£m báº£o ID duy nháº¥t cho má»—i document
    """
    print("\nğŸ“„ Converting to LangChain Documents...")
    
    langchain_docs = []
    seen_ids = set() # Set Ä‘á»ƒ kiá»ƒm tra ID trÃ¹ng láº·p
    
    for i, doc in enumerate(documents):
        doc_id = None # Khá»Ÿi táº¡o
        try:
            meta = doc.get("metadata", {}) 

            # ===== ğŸ”¥ FIX QUAN TRá»ŒNG: Táº O ID DUY NHáº¤T (Deduplication) =====
            # 1. Æ¯u tiÃªn: metadata['id'] -> 2. trÆ°á»ng 'id' chÃ­nh -> 3. Táº¡o temp ID
            doc_id = meta.get("id", doc.get('id', f"doc_{i}"))
            
            # Náº¿u ID Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng, thÃªm háº­u tá»‘ unique
            if doc_id in seen_ids:
                temp_id = f"{doc_id}_v{len(seen_ids)}"
                print(f"   âš ï¸ Duplicate ID '{doc_id}' detected at index {i}, using '{temp_id}'")
                doc_id = temp_id
            
            seen_ids.add(doc_id)
            # =============================================================

            def stringify_list(value):
                """Convert list to pipe-separated string"""
                if isinstance(value, list):
                    return "|".join(str(v) for v in value)
                if value is None:
                    return ""
                return str(value)

            # Chuáº©n bá»‹ Metadata (Láº¥y giÃ¡ trá»‹ máº·c Ä‘á»‹nh náº¿u thiáº¿u)
            metadata = {
                "id": doc_id, 
                "name": meta.get("name", "N/A"),
                "category": meta.get("category", "N/A"),
                "district": meta.get("district", "N/A"),
                "price_min": int(meta.get("price_min", 0)),
                "price_max": int(meta.get("price_max", 0)),
                "price_range": meta.get("price_range", "N/A"),
                "rating": float(meta.get("rating", 0.0)),
                "vibe_tags": stringify_list(meta.get("vibe_tags", [])),
                "specialties": stringify_list(meta.get("specialties", "")),
            }

            raw_content = doc.get('content', doc.get('page_content', ''))

            # ===== âœ… Tá»I Æ¯U: Táº O CONTENT Äáº¦Y Äá»¦ Äá»‚ EMBED (Content Augmentation) =====
            specialties_text = metadata["specialties"].replace("|", ", ") if metadata["specialties"] else ""
            
            enhanced_content = f"""TÃªn quÃ¡n: {metadata['name']}
Loáº¡i hÃ¬nh: {metadata['category']}
MÃ³n Ä‘áº·c sáº£n: {specialties_text}

{raw_content}"""

            langchain_doc = Document(
                page_content=enhanced_content, 
                metadata=metadata
            )

            langchain_docs.append(langchain_doc)

        except Exception as e:
            print(f"âš ï¸ Lá»—i xá»­ lÃ½ document táº¡i index {i}. ID: {doc_id}. Lá»—i: {e}")
            continue

    print(f"âœ… Converted {len(langchain_docs)} documents.")
    print(f"   (Sá»‘ ID duy nháº¥t Ä‘Æ°á»£c index: {len(seen_ids)})")
    
    if langchain_docs:
        sample = langchain_docs[0]
        print(f"\nğŸ“‹ Sample enhanced content:")
        print("â”€" * 60)
        print(sample.page_content[:300] + "...")
        print("â”€" * 60)
    
    return langchain_docs


def create_embeddings_model():
    """Táº¡o embedding model"""
    print("\nğŸ¤– Loading embedding model...")
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': EMBEDDING_DEVICE},
        encode_kwargs={'normalize_embeddings': True}
    )
    return embeddings


def create_vector_store(
    documents: List[Document],
    embeddings,
    persist_dir: Path
) -> Chroma:
    """Táº¡o vÃ  lÆ°u vector store"""
    print("\nğŸ—„ï¸ Creating vector store...")
    
    # XÃ³a DB cÅ© Ä‘á»ƒ táº¡o láº¡i vá»›i data Ä‘Ã£ gá»™p
    if persist_dir.exists():
        print(" Â  âš ï¸ Removing old database...")
        shutil.rmtree(persist_dir)
    
    # Táº¡o vector store
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=str(persist_dir),
        collection_name='restaurants',
        # Tá»‘i Æ°u hÃ³a cÃ i Ä‘áº·t HNSW cho hiá»‡u suáº¥t Cosine Similarity
        collection_metadata={"hnsw:space": "cosine"}
    )
    
    print(f"âœ… Vector store created! Total count: {vectorstore._collection.count()}")
    return vectorstore


def test_search(vectorstore: Chroma):
    """Test tÃ¬m kiáº¿m"""
    print("\n" + "="*60)
    print("ğŸ” TESTING VECTOR SEARCH")
    print("="*60)
    
    test_queries = [
        "TÃ¬m quÃ¡n phá»Ÿ ngon giÃ¡ ráº»", # Má»¥c tiÃªu test chÃ­nh
        "QuÃ¡n cafe yÃªn tÄ©nh Ä‘á»ƒ lÃ m viá»‡c",
        "BÃºn bÃ² Huáº¿ á»Ÿ Quáº­n 10",
    ]
    
    for query in test_queries:
        print(f"\nğŸ” Query: '{query}'")
        
        results = vectorstore.similarity_search_with_score(query, k=3)
        
        if not results:
            print(f" Â  âŒ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£!")
            continue
        
        print(f" Â  âœ… Found {len(results)} results:\n")
        
        for i, (doc, score) in enumerate(results, 1):
            print(f" Â  {i}. {doc.metadata.get('name', 'N/A')} ({doc.metadata.get('district', 'N/A')})")
            print(f" Â  Â  Â ğŸ“Š Distance: {score:.4f}")
            print(f" Â  Â  Â ğŸ·ï¸ {doc.metadata.get('category', 'N/A')}")
            
            if doc.metadata.get('specialties'):
                specs = doc.metadata['specialties'].split('|')[:3]
                print(f" Â  Â  Â ğŸœ MÃ³n: {', '.join(specs)}")
            print()


# ============= MAIN =============

def main():
    """Main function"""
    
    print("="*60)
    print("ğŸš€ CREATING VECTOR DATABASE - FINAL VERSION")
    print("="*60)
    
    try:
        # STEP 1: Load documents (tá»« file star.json Ä‘Ã£ gá»™p)
        documents = load_documents(JSON_FILE)
        
        # STEP 2: Convert to LangChain format (vá»›i ID duy nháº¥t vÃ  enhanced content)
        langchain_docs = create_langchain_documents(documents)
        
        # STEP 3: Create embedding model
        embeddings = create_embeddings_model()
        
        # STEP 4: Create vector store (XÃ³a DB cÅ© vÃ  táº¡o láº¡i)
        vectorstore = create_vector_store(
            langchain_docs,
            embeddings,
            VECTOR_DB_PATH
        )
        
        # STEP 5: Test search (Kiá»ƒm tra xem quÃ¡n Phá»Ÿ Ä‘Ã£ lÃªn chÆ°a)
        test_search(vectorstore)
        
        print("\n" + "="*60)
        print("âœ… SUCCESS! Vector database Ä‘Ã£ Ä‘Æ°á»£c táº¡o má»›i vá»›i ID duy nháº¥t.")
        print("="*60)
        print("\nğŸ’¡ BÃ¢y giá» hÃ£y cháº¡y láº¡i rag_system.py Ä‘á»ƒ test Hybrid Search (BM25 + Vector)!")
        
    except Exception as e:
        print(f"\nâŒ ERROR: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())